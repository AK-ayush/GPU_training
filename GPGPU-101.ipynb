{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK-ayush/GPU_training/blob/master/GPGPU-101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DjOJcffhWi",
        "colab_type": "text"
      },
      "source": [
        "## Getting started with General purpose GPU computing in python\n",
        "\n",
        "---\n",
        "<div align=\"center\"><img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12859-017-1666-0/MediaObjects/12859_2017_1666_Fig2_HTML.gif\" width=\"400\"/></div>\n",
        "\n",
        "*Left side- Thread organization*: Threads (red cubes) are organized in three-dimensional structures called blocks (yellow cubes), which belong to three-dimensional grid (green cube). The programmer must explicitly define the dimensions of blocks and grids. \n",
        "\n",
        "*Right side- Memory hierarchy*: In CUDA there are many different memories with different scopes. Each thread has two different kind of private memory: registers and local memories. Threads belonging to the same block can communicate through the shared memory, which has low access latency. The global memory suffers from high access latencies but it is accessible to all threads.\n",
        "\n",
        "[Courtesy: BMC Bioinformatics](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1666-0/figures/2)\n",
        "\n",
        "---\n",
        "<div align=\"center\"><img src=\"http://numba.pydata.org/_static/numba-blue-horizontal-rgb.svg\" width=\"400\"/></div>\n",
        "\n",
        "## Numba makes Python code fast\n",
        "\n",
        "CUDA has an execution model unlike the traditional sequential model used for programming CPUs. In CUDA, the code you write will be executed by multiple threads at once (often hundreds or thousands). **Your solution will be modeled by defining a thread hierarchy of grid, blocks and threads**.\n",
        "\n",
        "Numba’s CUDA support exposes facilities to declare and manage this hierarchy of threads. The facilities are largely similar to those exposed by NVidia’s CUDA C language.\n",
        "\n",
        "```python\n",
        "from numba import cuda\n",
        "import numpy as np\n",
        "\n",
        "# Kernel declaration:\n",
        "@cuda.jit\n",
        "def increment_by_one(d_array):\n",
        "    # Thread id in a 1D block\n",
        "    tx = cuda.threadIdx.x\n",
        "    # Block id in a 1D grid\n",
        "    ty = cuda.blockIdx.x\n",
        "    # Block width, i.e. number of threads per block\n",
        "    bw = cuda.blockDim.x\n",
        "    # Compute flattened index inside the array\n",
        "    pos = tx + ty * bw\n",
        "    if pos < d_array.size:  # Check array boundaries\n",
        "        d_array[pos] += 1\n",
        "\n",
        "#input array initialization\n",
        "h_array = np.arange(100)\n",
        "d_array = cuda.to_device(h_array)\n",
        "\n",
        "# A kernel is typically launched in the following way:\n",
        "threadsperblock = 32\n",
        "blockspergrid = (d_array.size + (threadsperblock - 1)) // threadsperblock\n",
        "increment_by_one[blockspergrid, threadsperblock](d_array)\n",
        "```\n",
        "[Courtesy: Numba pydata](http://numba.pydata.org/numba-doc/latest/cuda/kernels.html)\n",
        "\n",
        "---\n",
        "\n",
        "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/cupy/cupy/master/docs/image/cupy_logo_1000px.png\" width=\"400\"/></div>\n",
        "\n",
        "## CuPy : NumPy-like API accelerated with CUDA\n",
        "\n",
        "```python\n",
        "#cupy examples\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "x_gpu = cp.array([1, 2, 3])\n",
        "l2_gpu = cp.linalg.norm(x_gpu)\n",
        "\n",
        "# move the data across host and device\n",
        "x_cpu = np.array([1, 2, 3])\n",
        "x_gpu = cp.asarray(x_cpu)  # move the data to the current GPU device.\n",
        "\n",
        "x_cpu = cp.asnumpy(x_gpu)  # move the array to the host.\n",
        "```\n",
        "[Courtesy: CuPy Chainer](https://docs-cupy.chainer.org/en/stable/reference/index.html)\n",
        "\n",
        "---\n",
        "In this notebook, we are utilizing the GloVe word vectors dataset. Given a word, we are trying to find the nearest word from the rest of the words based on ecludian distance and cosine similarity respectively.\n",
        "\n",
        "This can be extended to image embeddings and other problems utilising vector space model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjtWsvkbyZoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import collections\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy14Ykuqs5P6",
        "colab_type": "code",
        "outputId": "34a85db7-7366-4b32-fb26-ccc3b06ff53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec  1 19:26:53 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.33.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mOdZWV-yo3M",
        "colab_type": "code",
        "outputId": "b2e3feef-b3cc-460e-a803-9c2d969d1feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# download glove vectors from web\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip #--quiet\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-01 19:13:03--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-12-01 19:13:03--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-12-01 19:13:03--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.08MB/s    in 6m 27s  \n",
            "\n",
            "2019-12-01 19:19:30 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-53ANVcysnK",
        "colab_type": "code",
        "outputId": "3aae6172-136e-4856-83e3-1b44eda53eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!head -1 glove.6B.100d.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wMyO7b8YCuI",
        "colab_type": "code",
        "outputId": "e48fbffc-cfa6-411f-d2c4-50b53347f88c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!wc -l glove.6B.100d.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000 glove.6B.100d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_jkf9iz1gV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def name_dtype_gen(dim_size=50):\n",
        "    col_types = collections.OrderedDict()\n",
        "    col_types['word'] = 'str'\n",
        "    for i in range(dim_size):\n",
        "        col_types['dim_'+str(i+1)]=np.float32\n",
        "    return col_types\n",
        "\n",
        "col_dtypes = name_dtype_gen(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWXcpFvg3E9o",
        "colab_type": "code",
        "outputId": "67ace8cc-1f3c-489a-8d56-fba6e0149cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time \n",
        "df = pd.read_csv(\"glove.6B.100d.txt\", delim_whitespace=True, names=col_dtypes.keys() , quoting=3) #ignore quoting\n",
        "df = df.astype(col_dtypes) \n",
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.58 s, sys: 695 ms, total: 7.28 s\n",
            "Wall time: 7.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBXFwEJN5VZ3",
        "colab_type": "code",
        "outputId": "c018cfa0-3cd1-4ae7-e1c3-c009f1da487a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        " df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>dim_1</th>\n",
              "      <th>dim_2</th>\n",
              "      <th>dim_3</th>\n",
              "      <th>dim_4</th>\n",
              "      <th>dim_5</th>\n",
              "      <th>dim_6</th>\n",
              "      <th>dim_7</th>\n",
              "      <th>dim_8</th>\n",
              "      <th>dim_9</th>\n",
              "      <th>dim_10</th>\n",
              "      <th>dim_11</th>\n",
              "      <th>dim_12</th>\n",
              "      <th>dim_13</th>\n",
              "      <th>dim_14</th>\n",
              "      <th>dim_15</th>\n",
              "      <th>dim_16</th>\n",
              "      <th>dim_17</th>\n",
              "      <th>dim_18</th>\n",
              "      <th>dim_19</th>\n",
              "      <th>dim_20</th>\n",
              "      <th>dim_21</th>\n",
              "      <th>dim_22</th>\n",
              "      <th>dim_23</th>\n",
              "      <th>dim_24</th>\n",
              "      <th>dim_25</th>\n",
              "      <th>dim_26</th>\n",
              "      <th>dim_27</th>\n",
              "      <th>dim_28</th>\n",
              "      <th>dim_29</th>\n",
              "      <th>dim_30</th>\n",
              "      <th>dim_31</th>\n",
              "      <th>dim_32</th>\n",
              "      <th>dim_33</th>\n",
              "      <th>dim_34</th>\n",
              "      <th>dim_35</th>\n",
              "      <th>dim_36</th>\n",
              "      <th>dim_37</th>\n",
              "      <th>dim_38</th>\n",
              "      <th>dim_39</th>\n",
              "      <th>...</th>\n",
              "      <th>dim_61</th>\n",
              "      <th>dim_62</th>\n",
              "      <th>dim_63</th>\n",
              "      <th>dim_64</th>\n",
              "      <th>dim_65</th>\n",
              "      <th>dim_66</th>\n",
              "      <th>dim_67</th>\n",
              "      <th>dim_68</th>\n",
              "      <th>dim_69</th>\n",
              "      <th>dim_70</th>\n",
              "      <th>dim_71</th>\n",
              "      <th>dim_72</th>\n",
              "      <th>dim_73</th>\n",
              "      <th>dim_74</th>\n",
              "      <th>dim_75</th>\n",
              "      <th>dim_76</th>\n",
              "      <th>dim_77</th>\n",
              "      <th>dim_78</th>\n",
              "      <th>dim_79</th>\n",
              "      <th>dim_80</th>\n",
              "      <th>dim_81</th>\n",
              "      <th>dim_82</th>\n",
              "      <th>dim_83</th>\n",
              "      <th>dim_84</th>\n",
              "      <th>dim_85</th>\n",
              "      <th>dim_86</th>\n",
              "      <th>dim_87</th>\n",
              "      <th>dim_88</th>\n",
              "      <th>dim_89</th>\n",
              "      <th>dim_90</th>\n",
              "      <th>dim_91</th>\n",
              "      <th>dim_92</th>\n",
              "      <th>dim_93</th>\n",
              "      <th>dim_94</th>\n",
              "      <th>dim_95</th>\n",
              "      <th>dim_96</th>\n",
              "      <th>dim_97</th>\n",
              "      <th>dim_98</th>\n",
              "      <th>dim_99</th>\n",
              "      <th>dim_100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>-0.038194</td>\n",
              "      <td>-0.244870</td>\n",
              "      <td>0.72812</td>\n",
              "      <td>-0.399610</td>\n",
              "      <td>0.083172</td>\n",
              "      <td>0.043953</td>\n",
              "      <td>-0.391410</td>\n",
              "      <td>0.334400</td>\n",
              "      <td>-0.57545</td>\n",
              "      <td>0.087459</td>\n",
              "      <td>0.287870</td>\n",
              "      <td>-0.06731</td>\n",
              "      <td>0.30906</td>\n",
              "      <td>-0.26384</td>\n",
              "      <td>-0.13231</td>\n",
              "      <td>-0.20757</td>\n",
              "      <td>0.33395</td>\n",
              "      <td>-0.338480</td>\n",
              "      <td>-0.31743</td>\n",
              "      <td>-0.48336</td>\n",
              "      <td>0.146400</td>\n",
              "      <td>-0.373040</td>\n",
              "      <td>0.34577</td>\n",
              "      <td>0.052041</td>\n",
              "      <td>0.449460</td>\n",
              "      <td>-0.469710</td>\n",
              "      <td>0.026280</td>\n",
              "      <td>-0.54155</td>\n",
              "      <td>-0.15518</td>\n",
              "      <td>-0.141070</td>\n",
              "      <td>-0.039722</td>\n",
              "      <td>0.28277</td>\n",
              "      <td>0.14393</td>\n",
              "      <td>0.234640</td>\n",
              "      <td>-0.31021</td>\n",
              "      <td>0.086173</td>\n",
              "      <td>0.20397</td>\n",
              "      <td>0.52624</td>\n",
              "      <td>0.171640</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.24978</td>\n",
              "      <td>0.92136</td>\n",
              "      <td>0.034514</td>\n",
              "      <td>0.46745</td>\n",
              "      <td>1.10790</td>\n",
              "      <td>-0.193580</td>\n",
              "      <td>-0.074575</td>\n",
              "      <td>0.233530</td>\n",
              "      <td>-0.052062</td>\n",
              "      <td>-0.220440</td>\n",
              "      <td>0.057162</td>\n",
              "      <td>-0.15806</td>\n",
              "      <td>-0.307980</td>\n",
              "      <td>-0.416250</td>\n",
              "      <td>0.379720</td>\n",
              "      <td>0.150060</td>\n",
              "      <td>-0.532120</td>\n",
              "      <td>-0.205500</td>\n",
              "      <td>-1.25260</td>\n",
              "      <td>0.071624</td>\n",
              "      <td>0.70565</td>\n",
              "      <td>0.497440</td>\n",
              "      <td>-0.42063</td>\n",
              "      <td>0.26148</td>\n",
              "      <td>-1.5380</td>\n",
              "      <td>-0.30223</td>\n",
              "      <td>-0.073438</td>\n",
              "      <td>-0.283120</td>\n",
              "      <td>0.371040</td>\n",
              "      <td>-0.25217</td>\n",
              "      <td>0.016215</td>\n",
              "      <td>-0.017099</td>\n",
              "      <td>-0.389840</td>\n",
              "      <td>0.87424</td>\n",
              "      <td>-0.72569</td>\n",
              "      <td>-0.51058</td>\n",
              "      <td>-0.520280</td>\n",
              "      <td>-0.14590</td>\n",
              "      <td>0.82780</td>\n",
              "      <td>0.270620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>,</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.110530</td>\n",
              "      <td>0.59812</td>\n",
              "      <td>-0.543610</td>\n",
              "      <td>0.673960</td>\n",
              "      <td>0.106630</td>\n",
              "      <td>0.038867</td>\n",
              "      <td>0.354810</td>\n",
              "      <td>0.06351</td>\n",
              "      <td>-0.094189</td>\n",
              "      <td>0.157860</td>\n",
              "      <td>-0.81665</td>\n",
              "      <td>0.14172</td>\n",
              "      <td>0.21939</td>\n",
              "      <td>0.58505</td>\n",
              "      <td>-0.52158</td>\n",
              "      <td>0.22783</td>\n",
              "      <td>-0.166420</td>\n",
              "      <td>-0.68228</td>\n",
              "      <td>0.35870</td>\n",
              "      <td>0.425680</td>\n",
              "      <td>0.190210</td>\n",
              "      <td>0.91963</td>\n",
              "      <td>0.575550</td>\n",
              "      <td>0.461850</td>\n",
              "      <td>0.423630</td>\n",
              "      <td>-0.095399</td>\n",
              "      <td>-0.42749</td>\n",
              "      <td>-0.16567</td>\n",
              "      <td>-0.056842</td>\n",
              "      <td>-0.295950</td>\n",
              "      <td>0.26037</td>\n",
              "      <td>-0.26606</td>\n",
              "      <td>-0.070404</td>\n",
              "      <td>-0.27662</td>\n",
              "      <td>0.158210</td>\n",
              "      <td>0.69825</td>\n",
              "      <td>0.43081</td>\n",
              "      <td>0.279520</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.22080</td>\n",
              "      <td>0.18669</td>\n",
              "      <td>0.131770</td>\n",
              "      <td>0.15117</td>\n",
              "      <td>0.71310</td>\n",
              "      <td>-0.352150</td>\n",
              "      <td>0.913480</td>\n",
              "      <td>0.617830</td>\n",
              "      <td>0.709920</td>\n",
              "      <td>0.239550</td>\n",
              "      <td>-0.145710</td>\n",
              "      <td>-0.37859</td>\n",
              "      <td>-0.045959</td>\n",
              "      <td>-0.473680</td>\n",
              "      <td>0.238500</td>\n",
              "      <td>0.205360</td>\n",
              "      <td>-0.189960</td>\n",
              "      <td>0.325070</td>\n",
              "      <td>-1.11120</td>\n",
              "      <td>-0.363410</td>\n",
              "      <td>0.98679</td>\n",
              "      <td>-0.084776</td>\n",
              "      <td>-0.54008</td>\n",
              "      <td>0.11726</td>\n",
              "      <td>-1.0194</td>\n",
              "      <td>-0.24424</td>\n",
              "      <td>0.127710</td>\n",
              "      <td>0.013884</td>\n",
              "      <td>0.080374</td>\n",
              "      <td>-0.35414</td>\n",
              "      <td>0.349510</td>\n",
              "      <td>-0.722600</td>\n",
              "      <td>0.375490</td>\n",
              "      <td>0.44410</td>\n",
              "      <td>-0.99059</td>\n",
              "      <td>0.61214</td>\n",
              "      <td>-0.351110</td>\n",
              "      <td>-0.83155</td>\n",
              "      <td>0.45293</td>\n",
              "      <td>0.082577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.</td>\n",
              "      <td>-0.339790</td>\n",
              "      <td>0.209410</td>\n",
              "      <td>0.46348</td>\n",
              "      <td>-0.647920</td>\n",
              "      <td>-0.383770</td>\n",
              "      <td>0.038034</td>\n",
              "      <td>0.171270</td>\n",
              "      <td>0.159780</td>\n",
              "      <td>0.46619</td>\n",
              "      <td>-0.019169</td>\n",
              "      <td>0.414790</td>\n",
              "      <td>-0.34349</td>\n",
              "      <td>0.26872</td>\n",
              "      <td>0.04464</td>\n",
              "      <td>0.42131</td>\n",
              "      <td>-0.41032</td>\n",
              "      <td>0.15459</td>\n",
              "      <td>0.022239</td>\n",
              "      <td>-0.64653</td>\n",
              "      <td>0.25256</td>\n",
              "      <td>0.043136</td>\n",
              "      <td>-0.194450</td>\n",
              "      <td>0.46516</td>\n",
              "      <td>0.456510</td>\n",
              "      <td>0.685880</td>\n",
              "      <td>0.091295</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>-0.70351</td>\n",
              "      <td>0.16785</td>\n",
              "      <td>-0.350790</td>\n",
              "      <td>-0.126340</td>\n",
              "      <td>0.66384</td>\n",
              "      <td>-0.25820</td>\n",
              "      <td>0.036542</td>\n",
              "      <td>-0.13605</td>\n",
              "      <td>0.402530</td>\n",
              "      <td>0.14289</td>\n",
              "      <td>0.38132</td>\n",
              "      <td>-0.122830</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.55262</td>\n",
              "      <td>0.65000</td>\n",
              "      <td>0.086426</td>\n",
              "      <td>0.39012</td>\n",
              "      <td>1.06320</td>\n",
              "      <td>-0.353790</td>\n",
              "      <td>0.483280</td>\n",
              "      <td>0.346000</td>\n",
              "      <td>0.841740</td>\n",
              "      <td>0.098707</td>\n",
              "      <td>-0.242130</td>\n",
              "      <td>-0.27053</td>\n",
              "      <td>0.045287</td>\n",
              "      <td>-0.401470</td>\n",
              "      <td>0.113950</td>\n",
              "      <td>0.006223</td>\n",
              "      <td>0.036673</td>\n",
              "      <td>0.018518</td>\n",
              "      <td>-1.02130</td>\n",
              "      <td>-0.208060</td>\n",
              "      <td>0.64072</td>\n",
              "      <td>-0.068763</td>\n",
              "      <td>-0.58635</td>\n",
              "      <td>0.33476</td>\n",
              "      <td>-1.1432</td>\n",
              "      <td>-0.11480</td>\n",
              "      <td>-0.250910</td>\n",
              "      <td>-0.459070</td>\n",
              "      <td>-0.096819</td>\n",
              "      <td>-0.17946</td>\n",
              "      <td>-0.063351</td>\n",
              "      <td>-0.674120</td>\n",
              "      <td>-0.068895</td>\n",
              "      <td>0.53604</td>\n",
              "      <td>-0.87773</td>\n",
              "      <td>0.31802</td>\n",
              "      <td>-0.392420</td>\n",
              "      <td>-0.23394</td>\n",
              "      <td>0.47298</td>\n",
              "      <td>-0.028803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of</td>\n",
              "      <td>-0.152900</td>\n",
              "      <td>-0.242790</td>\n",
              "      <td>0.89837</td>\n",
              "      <td>0.169960</td>\n",
              "      <td>0.535160</td>\n",
              "      <td>0.487840</td>\n",
              "      <td>-0.588260</td>\n",
              "      <td>-0.179820</td>\n",
              "      <td>-1.35810</td>\n",
              "      <td>0.425410</td>\n",
              "      <td>0.153770</td>\n",
              "      <td>0.24215</td>\n",
              "      <td>0.13474</td>\n",
              "      <td>0.41193</td>\n",
              "      <td>0.67043</td>\n",
              "      <td>-0.56418</td>\n",
              "      <td>0.42985</td>\n",
              "      <td>-0.012183</td>\n",
              "      <td>-0.11677</td>\n",
              "      <td>0.31781</td>\n",
              "      <td>0.054177</td>\n",
              "      <td>-0.054273</td>\n",
              "      <td>0.35516</td>\n",
              "      <td>-0.302410</td>\n",
              "      <td>0.314340</td>\n",
              "      <td>-0.338460</td>\n",
              "      <td>0.717150</td>\n",
              "      <td>-0.26855</td>\n",
              "      <td>-0.15837</td>\n",
              "      <td>-0.474670</td>\n",
              "      <td>0.051581</td>\n",
              "      <td>-0.33252</td>\n",
              "      <td>0.15003</td>\n",
              "      <td>-0.129900</td>\n",
              "      <td>-0.54617</td>\n",
              "      <td>-0.378430</td>\n",
              "      <td>0.64261</td>\n",
              "      <td>0.82187</td>\n",
              "      <td>-0.080006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04885</td>\n",
              "      <td>0.78267</td>\n",
              "      <td>0.384970</td>\n",
              "      <td>0.42097</td>\n",
              "      <td>0.67882</td>\n",
              "      <td>0.103370</td>\n",
              "      <td>0.632800</td>\n",
              "      <td>-0.026595</td>\n",
              "      <td>0.586470</td>\n",
              "      <td>-0.443320</td>\n",
              "      <td>0.330570</td>\n",
              "      <td>-0.12022</td>\n",
              "      <td>-0.556450</td>\n",
              "      <td>0.073611</td>\n",
              "      <td>0.209150</td>\n",
              "      <td>0.433950</td>\n",
              "      <td>-0.012761</td>\n",
              "      <td>0.089874</td>\n",
              "      <td>-1.79910</td>\n",
              "      <td>0.084808</td>\n",
              "      <td>0.77112</td>\n",
              "      <td>0.631050</td>\n",
              "      <td>-0.90685</td>\n",
              "      <td>0.60326</td>\n",
              "      <td>-1.7515</td>\n",
              "      <td>0.18596</td>\n",
              "      <td>-0.506870</td>\n",
              "      <td>-0.702030</td>\n",
              "      <td>0.665780</td>\n",
              "      <td>-0.81304</td>\n",
              "      <td>0.187120</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>-0.267570</td>\n",
              "      <td>0.72700</td>\n",
              "      <td>-0.59363</td>\n",
              "      <td>-0.34839</td>\n",
              "      <td>-0.560940</td>\n",
              "      <td>-0.59100</td>\n",
              "      <td>1.00390</td>\n",
              "      <td>0.206640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>to</td>\n",
              "      <td>-0.189700</td>\n",
              "      <td>0.050024</td>\n",
              "      <td>0.19084</td>\n",
              "      <td>-0.049184</td>\n",
              "      <td>-0.089737</td>\n",
              "      <td>0.210060</td>\n",
              "      <td>-0.549520</td>\n",
              "      <td>0.098377</td>\n",
              "      <td>-0.20135</td>\n",
              "      <td>0.342410</td>\n",
              "      <td>-0.092677</td>\n",
              "      <td>0.16100</td>\n",
              "      <td>-0.13268</td>\n",
              "      <td>-0.28160</td>\n",
              "      <td>0.18737</td>\n",
              "      <td>-0.42959</td>\n",
              "      <td>0.96039</td>\n",
              "      <td>0.139720</td>\n",
              "      <td>-1.07810</td>\n",
              "      <td>0.40518</td>\n",
              "      <td>0.505390</td>\n",
              "      <td>-0.550640</td>\n",
              "      <td>0.48440</td>\n",
              "      <td>0.380440</td>\n",
              "      <td>-0.002905</td>\n",
              "      <td>-0.349420</td>\n",
              "      <td>-0.099696</td>\n",
              "      <td>-0.78368</td>\n",
              "      <td>1.03630</td>\n",
              "      <td>-0.231400</td>\n",
              "      <td>-0.471210</td>\n",
              "      <td>0.57126</td>\n",
              "      <td>-0.21454</td>\n",
              "      <td>0.359580</td>\n",
              "      <td>-0.48319</td>\n",
              "      <td>1.087500</td>\n",
              "      <td>0.28524</td>\n",
              "      <td>0.12447</td>\n",
              "      <td>-0.039248</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.34780</td>\n",
              "      <td>0.51621</td>\n",
              "      <td>-0.433870</td>\n",
              "      <td>0.36852</td>\n",
              "      <td>0.74573</td>\n",
              "      <td>0.072102</td>\n",
              "      <td>0.279310</td>\n",
              "      <td>0.925690</td>\n",
              "      <td>-0.050336</td>\n",
              "      <td>-0.858560</td>\n",
              "      <td>-0.135800</td>\n",
              "      <td>-0.92551</td>\n",
              "      <td>-0.339910</td>\n",
              "      <td>-1.039400</td>\n",
              "      <td>-0.067203</td>\n",
              "      <td>-0.213790</td>\n",
              "      <td>-0.476900</td>\n",
              "      <td>0.213770</td>\n",
              "      <td>-0.84008</td>\n",
              "      <td>0.052536</td>\n",
              "      <td>0.59298</td>\n",
              "      <td>0.296040</td>\n",
              "      <td>-0.67644</td>\n",
              "      <td>0.13916</td>\n",
              "      <td>-1.5504</td>\n",
              "      <td>-0.20765</td>\n",
              "      <td>0.722200</td>\n",
              "      <td>0.520560</td>\n",
              "      <td>-0.076221</td>\n",
              "      <td>-0.15194</td>\n",
              "      <td>-0.131340</td>\n",
              "      <td>0.058617</td>\n",
              "      <td>-0.318690</td>\n",
              "      <td>-0.61419</td>\n",
              "      <td>-0.62393</td>\n",
              "      <td>-0.41548</td>\n",
              "      <td>-0.038175</td>\n",
              "      <td>-0.39804</td>\n",
              "      <td>0.47647</td>\n",
              "      <td>-0.159830</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  word     dim_1     dim_2    dim_3  ...    dim_97   dim_98   dim_99   dim_100\n",
              "0  the -0.038194 -0.244870  0.72812  ... -0.520280 -0.14590  0.82780  0.270620\n",
              "1    , -0.107670  0.110530  0.59812  ... -0.351110 -0.83155  0.45293  0.082577\n",
              "2    . -0.339790  0.209410  0.46348  ... -0.392420 -0.23394  0.47298 -0.028803\n",
              "3   of -0.152900 -0.242790  0.89837  ... -0.560940 -0.59100  1.00390  0.206640\n",
              "4   to -0.189700  0.050024  0.19084  ... -0.038175 -0.39804  0.47647 -0.159830\n",
              "\n",
              "[5 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebAw75PtX5OY",
        "colab_type": "code",
        "outputId": "52d25a56-e8b1-448d-c3ef-980916a27188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIj4GzTv3T-4",
        "colab_type": "code",
        "outputId": "165086ee-e895-4c81-a390-ff50cfd875b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mat = df.loc[df['word']!='queen', df.columns!='word'].to_numpy() \n",
        "test_vec = df.loc[df['word']=='queen', df.columns!='word'].to_numpy()\n",
        "df = df.loc[df['word']!='queen']\n",
        "words_arr = df.word.to_numpy()\n",
        "mat.shape, test_vec.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((399999, 100), (1, 100))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i77VH3vgkEs5",
        "colab_type": "code",
        "outputId": "4188eb32-2104-4bde-d1ad-c5c398fa07c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(399999, 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1tQwZPflHdS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Ecludian Distance:\n",
        "<div align=\"center\"><img src=\"http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/sphilip/images/euclid_eqn.gif\" width=\"300\"/></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvShnB8h-zIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ecludean_dist(a,b, dim_size):\n",
        "    summ = 0\n",
        "    for i in range(dim_size):\n",
        "        summ += ((a[i]-b[i])**2)\n",
        "    return math.sqrt(summ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxO03VklaTV",
        "colab_type": "text"
      },
      "source": [
        "### Cosine Similarity:\n",
        "<div align=\"center\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d\" width=\"400\"/></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "076lEw0OBi5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dot(a, b, dim_size):\n",
        "    summ = 0\n",
        "    for i in range(dim_size):\n",
        "        summ += (a[i]*b[i])\n",
        "    return summ\n",
        "\n",
        "def cosine_sim(a, b, dim_size):\n",
        "    return dot(a,b, dim_size) / ( math.sqrt(dot(a, a, dim_size)) * math.sqrt(dot(b, b, dim_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLG7AbWlB055",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_nearest(mat, vec, dim_size, n):\n",
        "    ret_eldn = np.array([ecludean_dist(vec, mat[idx], dim_size) for idx in range(n)])\n",
        "    ret_csn = np.array([cosine_sim(vec, mat[idx], dim_size) for idx in range(n)])\n",
        "    return ret_eldn, ret_csn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-zcOMKiEUcU",
        "colab_type": "code",
        "outputId": "d50c7c62-e066-4d44-8d74-ab8c68b309ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time \n",
        "ret_eldn, ret_csn = find_nearest(mat, test_vec[0], mat.shape[1], mat.shape[0])\n",
        "i1, i2 = np.argpartition(ret_eldn, 5)[:5], np.argpartition(ret_csn, -5)[-5:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 50s, sys: 368 ms, total: 3min 50s\n",
            "Wall time: 3min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZOnFZUboYcz",
        "colab_type": "code",
        "outputId": "13339184-93e6-4b10-a635-bc77d08ec527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "i1,i2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 691, 4241, 3217, 2790, 3672]), array([2790, 4241, 3217, 1142,  691]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue5whrXLeWO9",
        "colab_type": "code",
        "outputId": "bf76eaa1-2af0-45bc-c554-19e58037c9ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Given word: \", 'queen')\n",
        "print(\"Nearest using Ecludian: \", words_arr[i1])\n",
        "print(\"Nearest using Cosine: \", words_arr[i2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given word:  queen\n",
            "Nearest using Ecludian:  ['king' 'princess' 'elizabeth' 'lady' 'victoria']\n",
            "Nearest using Cosine:  ['lady' 'princess' 'elizabeth' 'royal' 'king']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCumIXYe4Czr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del ret_eldn, ret_csn, i1, i2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpKaohOFFCAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numba import cuda\n",
        "import cupy as cp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xam9-VJYWFY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"NUMBAPRO_NVVM\"]=\"/usr/local/cuda/nvvm/lib64/libnvvm.so\"\n",
        "os.environ[\"NUMBAPRO_LIBDEVICE\"]=\"/usr/local/cuda/nvvm/libdevice/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6g6oyseSlef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim_size=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0irR7IMHDGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@cuda.jit(device=True)\n",
        "def ecludean_dist(a,b, dim_size):\n",
        "    summ = 0\n",
        "    for i in range(dim_size):\n",
        "        summ += ((a[i]-b[i])**2)\n",
        "    return math.sqrt(summ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApxRCS8WHHU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@cuda.jit(device=True)\n",
        "def dot(a, b, dim_size):\n",
        "    summ = 0\n",
        "    for i in range(dim_size):\n",
        "        summ += (a[i]*b[i])\n",
        "    return summ\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def cosine_sim(a, b, dim_size):\n",
        "    return dot(a,b, dim_size) / ( math.sqrt(dot(a, a, dim_size)) * math.sqrt(dot(b, b, dim_size)) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4TZz3qgHRIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@cuda.jit('void(float32[:,:], float32[:], float32[:], float32[:], int32, int32)')\n",
        "def find_nearest_gpu(d_mat, d_vec, ret_eldn, ret_csn, dim_size, n):\n",
        "    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "    if idx < n:  \n",
        "      ret_eldn[idx] = ecludean_dist(d_vec, d_mat[idx], dim_size)\n",
        "      ret_csn[idx] = cosine_sim(d_vec, d_mat[idx], dim_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUcYnj1RSdG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = mat.shape[0]\n",
        "\n",
        "d_mat = cp.asarray(mat)\n",
        "d_vec = cp.asarray(test_vec[0])\n",
        "ret_eldn = cp.zeros(shape=n, dtype=np.float32)  \n",
        "ret_csn = cp.zeros(shape=n, dtype=np.float32) \n",
        "\n",
        "# d_mat = cuda.to_device(mat)\n",
        "# d_vec = cuda.to_device(test_vecs)\n",
        "# ret_eldn = cuda.device_array(shape=n, dtype=np.float32)\n",
        "# ret_csn = cuda.device_array(shape=n, dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mVshd_0YTFF",
        "colab_type": "code",
        "outputId": "3a1a4441-8592-47f8-8641-26079da10ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = cuda.get_current_device()\n",
        "\n",
        "tpb = 64#device.WARP_SIZE    #blocksize or thread per block\n",
        "bpg = int(np.ceil((n)/tpb))  # block per grid\n",
        "(tpb, bpg)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 6250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0bM_h1Pscf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLTEDgkiYhW-",
        "colab_type": "code",
        "outputId": "3b89bf58-d8a2-470c-b5e6-efe0aa34f4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "find_nearest_gpu[bpg,tpb](d_mat, d_vec, ret_eldn, ret_csn, dim_size, n)\n",
        "cuda.synchronize()\n",
        "i1, i2 = cp.argpartition(ret_eldn, 5)[:5], cp.argpartition(ret_csn, -5)[-5:]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 320 ms, sys: 5.05 ms, total: 325 ms\n",
            "Wall time: 442 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5LcP0d6lkFD",
        "colab_type": "code",
        "outputId": "0c76f871-15e9-4d32-88f3-73e5277ccf59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "i1, i2"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4241, 3217,  691, 2790, 3672]), array([2790, 1142, 3217,  691, 4241]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrpOAr9XFEGE",
        "colab_type": "code",
        "outputId": "8274712a-8969-493e-9d79-516e0ab9703f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Given word: \", 'queen')\n",
        "print(\"Nearest using Ecludian: \", words_arr[i1.tolist()])\n",
        "print(\"Nearest using Cosine: \", words_arr[i2.tolist()])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given word:  queen\n",
            "Nearest using Ecludian:  ['princess' 'elizabeth' 'king' 'lady' 'victoria']\n",
            "Nearest using Cosine:  ['lady' 'royal' 'elizabeth' 'king' 'princess']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FUJHQN8_kZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}